{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应`tf.kears` 版本的03，在训练过程中加入更多的控制\n",
    "\n",
    "1. 训练中保存/保存最好的模型\n",
    "2. 早停 \n",
    "3. 训练过程可视化\n",
    "\n",
    "<font color=\"red\">注</font>: 使用 tensorboard 可视化需要安装 tensorflow (TensorBoard依赖于tensorflow库，可以任意安装tensorflow的gpu/cpu版本)\n",
    "\n",
    "```shell\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:00.097561Z",
     "start_time": "2025-07-26T19:29:59.752371Z"
    }
   },
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn,tf, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "    \n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)  #设备是cuda:0，即GPU，如果没有GPU则是cpu\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0)\n",
      "matplotlib 3.8.3\n",
      "numpy 1.26.4\n",
      "pandas 2.3.1\n",
      "sklearn 1.7.1\n",
      "tensorflow 2.16.1\n",
      "torch 2.2.1+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:02.457857Z",
     "start_time": "2025-07-26T19:30:00.151421Z"
    }
   },
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# fashion_mnist图像分类数据集\n",
    "train_ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# torchvision 数据集里没有提供训练集和验证集的划分\n",
    "# 当然也可以用 torch.utils.data.Dataset 实现人为划分"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.077827Z",
     "start_time": "2025-07-26T19:30:03.028551Z"
    }
   },
   "source": [
    "# 从数据集到dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "source": [
    "# 查看数据\n",
    "for datas, labels in train_loader:\n",
    "    print(datas.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "#查看val_loader\n",
    "for datas, labels in val_loader:\n",
    "    print(datas.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.296419Z",
     "start_time": "2025-07-26T19:30:03.167128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.404797Z",
     "start_time": "2025-07-26T19:30:03.327395Z"
    }
   },
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 300),  # in_features=784, out_features=300\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape [batch size, 1, 28, 28]\n",
    "        x = self.flatten(x)  \n",
    "        # 展平后 x.shape [batch size, 28 * 28]\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # logits.shape [batch size, 10]\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "pytorch的训练需要自行实现，包括\n",
    "1. 定义损失函数\n",
    "2. 定义优化器\n",
    "3. 定义训练步\n",
    "4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.467934Z",
     "start_time": "2025-07-26T19:30:03.439643Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    for datas, labels in dataloader:\n",
    "        #datas.shape [batch size, 1, 28, 28]\n",
    "        #labels.shape [batch size]\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 前向计算\n",
    "        logits = model(datas)\n",
    "        loss = loss_fct(logits, labels)         # 验证集损失\n",
    "        loss_list.append(loss.item()) # tensor.item() 获取tensor的数值，loss是只有一个元素的tensor\n",
    "        \n",
    "        preds = logits.argmax(axis=-1)    # 验证集预测, axis=-1 表示最后一个维度,因为logits.shape [batch size, 10]，所以axis=-1表示对最后一个维度求argmax，即对每个样本的10个类别的概率求argmax，得到最大概率的类别, preds.shape [batch size]\n",
    "        pred_list.extend(preds.cpu().numpy().tolist()) # tensor转numpy，再转list\n",
    "        label_list.extend(labels.cpu().numpy().tolist())\n",
    "        \n",
    "    acc = accuracy_score(label_list, pred_list) # 验证集准确率\n",
    "    return np.mean(loss_list), acc # 返回验证集平均损失和准确率\n"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chapter_2_torch# TensorBoard 可视化\n",
    "\n",
    "pip install tensorboard\n",
    "训练过程中可以使用如下命令启动tensorboard服务。注意使用绝对路径，否则会报错\n",
    "\n",
    "```shell\n",
    " tensorboard  --logdir=\"D:\\BaiduSyncdisk\\pytorch\\chapter_2_torch\\runs\" --host 0.0.0.0 --port 8848\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.593706Z",
     "start_time": "2025-07-26T19:30:03.500563Z"
    }
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class TensorBoardCallback:# 定义tensorboard回调函数, 用于可视化训练过程\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_dir (str): dir to write log.\n",
    "            flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs) # 实例化SummaryWriter, log_dir是log存放路径，flush_secs是每隔多少秒写入磁盘\n",
    "\n",
    "    def draw_model(self, model, input_shape):#graphs\n",
    "        self.writer.add_graph(model, input_to_model=torch.randn(input_shape)) # 画模型图\n",
    "        \n",
    "    def add_loss_scalars(self, step, loss, val_loss):#增加loss曲线\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/loss\", \n",
    "            tag_scalar_dict={\"loss\": loss, \"val_loss\": val_loss},\n",
    "            global_step=step,\n",
    "            ) # 画loss曲线, main_tag是主tag，tag_scalar_dict是子tag，global_step是步数\n",
    "        \n",
    "    def add_acc_scalars(self, step, acc, val_acc):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/accuracy\",\n",
    "            tag_scalar_dict={\"accuracy\": acc, \"val_accuracy\": val_acc},\n",
    "            global_step=step,\n",
    "        ) # 画acc曲线, main_tag是主tag，tag_scalar_dict是子tag，global_step是步数\n",
    "        \n",
    "    def add_lr_scalars(self, step, learning_rate):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/learning_rate\",\n",
    "            tag_scalar_dict={\"learning_rate\": learning_rate},\n",
    "            global_step=step,\n",
    "        ) # 画lr曲线, main_tag是主tag，tag_scalar_dict是子tag，global_step是步数\n",
    "    \n",
    "    def __call__(self, step, **kwargs):\n",
    "        # add loss,把loss，val_loss取掉，画loss曲线\n",
    "        loss = kwargs.pop(\"loss\", None)\n",
    "        val_loss = kwargs.pop(\"val_loss\", None)\n",
    "        if loss is not None and val_loss is not None:\n",
    "            self.add_loss_scalars(step, loss, val_loss) # 画loss曲线\n",
    "        # add acc\n",
    "        acc = kwargs.pop(\"acc\", None)\n",
    "        val_acc = kwargs.pop(\"val_acc\", None)\n",
    "        if acc is not None and val_acc is not None:\n",
    "            self.add_acc_scalars(step, acc, val_acc) # 画acc曲线\n",
    "        # add lr\n",
    "        learning_rate = kwargs.pop(\"lr\", None)\n",
    "        if learning_rate is not None:\n",
    "            self.add_lr_scalars(step, learning_rate) # 画lr曲线\n"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Best\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.657600Z",
     "start_time": "2025-07-26T19:30:03.628697Z"
    }
   },
   "source": [
    "class SaveCheckpointsCallback:\n",
    "    def __init__(self, save_dir, save_step=500, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Save checkpoints each save_epoch epoch. \n",
    "        We save checkpoint by epoch in this implementation.\n",
    "        Usually, training scripts with pytorch evaluating model and save checkpoint by step.\n",
    "\n",
    "        Args:\n",
    "            save_dir (str): dir to save checkpoint\n",
    "            save_epoch (int, optional): the frequency to save checkpoint. Defaults to 1.\n",
    "            save_best_only (bool, optional): If True, only save the best model or save each model at every epoch.\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir # 保存路径\n",
    "        self.save_step = save_step # 保存步数\n",
    "        self.save_best_only = save_best_only # 是否只保存最好的模型\n",
    "        self.best_metrics = -1 # 最好的指标，指标不可能为负数，所以初始化为-1\n",
    "        \n",
    "        # mkdir\n",
    "        if not os.path.exists(self.save_dir): # 如果不存在保存路径，则创建\n",
    "            os.mkdir(self.save_dir)\n",
    "        \n",
    "    def __call__(self, step, state_dict, metric=None):\n",
    "        if step % self.save_step > 0: #每隔save_step步保存一次\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            assert metric is not None # 必须传入metric\n",
    "            if metric >= self.best_metrics:\n",
    "                # save checkpoints\n",
    "                torch.save(state_dict, os.path.join(self.save_dir, \"best.ckpt\")) # 保存最好的模型，覆盖之前的模型，不保存step，只保存state_dict，即模型参数，不保存优化器参数\n",
    "                # update best metrics\n",
    "                self.best_metrics = metric\n",
    "        else:\n",
    "            torch.save(state_dict, os.path.join(self.save_dir, f\"{step}.ckpt\")) # 保存每个step的模型，不覆盖之前的模型，保存step，保存state_dict，即模型参数，不保存优化器参数\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.708196Z",
     "start_time": "2025-07-26T19:30:03.686925Z"
    }
   },
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            patience (int, optional): Number of epochs with no improvement after which training will be stopped.. Defaults to 5.\n",
    "            min_delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute \n",
    "                change of less than min_delta, will count as no improvement. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.patience = patience # 多少个epoch没有提升就停止训练\n",
    "        self.min_delta = min_delta # 最小的提升幅度\n",
    "        self.best_metric = -1\n",
    "        self.counter = 0\n",
    "        \n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:#用准确率\n",
    "            # update best metric\n",
    "            self.best_metric = metric\n",
    "            # reset counter \n",
    "            self.counter = 0\n",
    "        else: \n",
    "            self.counter += 1 # 计数器加1，下面的patience判断用到\n",
    "            \n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "source": [
    "500*32*5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:03.791808Z",
     "start_time": "2025-07-26T19:30:03.735851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:04.076952Z",
     "start_time": "2025-07-26T19:30:03.826566Z"
    }
   },
   "source": [
    "# 训练\n",
    "def training(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    epoch, \n",
    "    loss_fct, \n",
    "    optimizer, \n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "    \n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_loader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            # training\n",
    "            for datas, labels in train_loader:\n",
    "                datas = datas.to(device) # 数据放到device上\n",
    "                labels = labels.to(device) # 标签放到device上\n",
    "                # 梯度清空\n",
    "                optimizer.zero_grad()\n",
    "                # 模型前向计算\n",
    "                logits = model(datas)\n",
    "                # 计算损失\n",
    "                loss = loss_fct(logits, labels)\n",
    "                # 梯度回传，计算梯度，更新参数，这里是更新模型参数\n",
    "                loss.backward()\n",
    "                # 调整优化器，包括学习率的变动等\n",
    "                optimizer.step()\n",
    "                preds = logits.argmax(axis=-1)\n",
    "            \n",
    "                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())    \n",
    "                loss = loss.cpu().item()\n",
    "                # record\n",
    "                \n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"acc\": acc, \"step\": global_step\n",
    "                })\n",
    "                \n",
    "                # evaluating\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()  # 切换到验证集模式\n",
    "                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)\n",
    "                    record_dict[\"val\"].append({\n",
    "                        \"loss\": val_loss, \"acc\": val_acc, \"step\": global_step\n",
    "                    })\n",
    "                    model.train() # 切换回训练集模式\n",
    "                    \n",
    "                    # 1. 使用 tensorboard 可视化\n",
    "                    if tensorboard_callback is not None:\n",
    "                        tensorboard_callback(\n",
    "                            global_step, \n",
    "                            loss=loss, val_loss=val_loss,\n",
    "                            acc=acc, val_acc=val_acc,\n",
    "                            lr=optimizer.param_groups[0][\"lr\"], # 取出当前学习率\n",
    "                            )\n",
    "                    \n",
    "                    # 2. 保存模型权重 save model checkpoint\n",
    "                    if save_ckpt_callback is not None:\n",
    "                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc) # 保存最好的模型，覆盖之前的模型，保存step，保存state_dict,通过metric判断是否保存最好的模型\n",
    "\n",
    "                    # 3. 早停 Early Stop\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(val_acc) # 验证集准确率不再提升，则停止训练\n",
    "                        if early_stop_callback.early_stop:# 验证集准确率不再提升，则停止训练\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "                    \n",
    "                # udate step\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"epoch\": epoch_id})\n",
    "        \n",
    "    return record_dict"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:04.114938Z",
     "start_time": "2025-07-26T19:30:04.104821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.listdir(\"runs\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['events.out.tfevents.1753534272.DESKTOP-51Q3DND.24284.0']\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "source": [
    "epoch = 100\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# 1. 定义损失函数 采用MSE损失\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "# 2. 定义优化器 采用SGD\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 1. tensorboard 可视化\n",
    "tensorboard_callback = TensorBoardCallback(\"runs\")\n",
    "tensorboard_callback.draw_model(model, [1, 28, 28])\n",
    "# 2. save best\n",
    "save_ckpt_callback = SaveCheckpointsCallback(\"checkpoints\", save_best_only=True)\n",
    "# 3. early stop\n",
    "early_stop_callback = EarlyStopCallback(patience=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:05.903085Z",
     "start_time": "2025-07-26T19:30:04.163973Z"
    }
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "runs is not a directory",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFailedPreconditionError\u001B[39m                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      9\u001B[39m optimizer = torch.optim.SGD(model.parameters(), lr=\u001B[32m0.001\u001B[39m, momentum=\u001B[32m0.9\u001B[39m)\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# 1. tensorboard 可视化\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m tensorboard_callback = \u001B[43mTensorBoardCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mruns\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m tensorboard_callback.draw_model(model, [\u001B[32m1\u001B[39m, \u001B[32m28\u001B[39m, \u001B[32m28\u001B[39m])\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# 2. save best\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[88]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mTensorBoardCallback.__init__\u001B[39m\u001B[34m(self, log_dir, flush_secs)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, log_dir, flush_secs=\u001B[32m10\u001B[39m):\n\u001B[32m      5\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[33;03m        log_dir (str): dir to write log.\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[33;03m        flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     \u001B[38;5;28mself\u001B[39m.writer = \u001B[43mSummaryWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflush_secs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mflush_secs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:249\u001B[39m, in \u001B[36mSummaryWriter.__init__\u001B[39m\u001B[34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;66;03m# Initialize the file writers, but they can be cleared out on close\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[38;5;66;03m# and recreated later as needed.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[38;5;28mself\u001B[39m.file_writer = \u001B[38;5;28mself\u001B[39m.all_writers = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m249\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_file_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;66;03m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001B[39;00m\n\u001B[32m    252\u001B[39m v = \u001B[32m1e-12\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:281\u001B[39m, in \u001B[36mSummaryWriter._get_file_writer\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    279\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return the default FileWriter instance. Recreates it if closed.\"\"\"\u001B[39;00m\n\u001B[32m    280\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.all_writers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.file_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m281\u001B[39m     \u001B[38;5;28mself\u001B[39m.file_writer = \u001B[43mFileWriter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    282\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mflush_secs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfilename_suffix\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    284\u001B[39m     \u001B[38;5;28mself\u001B[39m.all_writers = {\u001B[38;5;28mself\u001B[39m.file_writer.get_logdir(): \u001B[38;5;28mself\u001B[39m.file_writer}\n\u001B[32m    285\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.purge_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:75\u001B[39m, in \u001B[36mFileWriter.__init__\u001B[39m\u001B[34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[38;5;66;03m# Sometimes PosixPath is passed in and we need to coerce it to\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[38;5;66;03m# a string in all cases\u001B[39;00m\n\u001B[32m     72\u001B[39m \u001B[38;5;66;03m# TODO: See if we can remove this in the future if we are\u001B[39;00m\n\u001B[32m     73\u001B[39m \u001B[38;5;66;03m# actually the ones passing in a PosixPath\u001B[39;00m\n\u001B[32m     74\u001B[39m log_dir = \u001B[38;5;28mstr\u001B[39m(log_dir)\n\u001B[32m---> \u001B[39m\u001B[32m75\u001B[39m \u001B[38;5;28mself\u001B[39m.event_writer = \u001B[43mEventFileWriter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     76\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflush_secs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename_suffix\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py:72\u001B[39m, in \u001B[36mEventFileWriter.__init__\u001B[39m\u001B[34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001B[39m\n\u001B[32m     57\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Creates a `EventFileWriter` and an event file to write to.\u001B[39;00m\n\u001B[32m     58\u001B[39m \n\u001B[32m     59\u001B[39m \u001B[33;03mOn construction the summary writer creates a new event file in `logdir`.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     69\u001B[39m \u001B[33;03m    pending events and summaries to disk.\u001B[39;00m\n\u001B[32m     70\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[38;5;28mself\u001B[39m._logdir = logdir\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m \u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgfile\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogdir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[38;5;28mself\u001B[39m._file_name = (\n\u001B[32m     74\u001B[39m     os.path.join(\n\u001B[32m     75\u001B[39m         logdir,\n\u001B[32m   (...)\u001B[39m\u001B[32m     84\u001B[39m     + filename_suffix\n\u001B[32m     85\u001B[39m )  \u001B[38;5;66;03m# noqa E128\u001B[39;00m\n\u001B[32m     86\u001B[39m \u001B[38;5;28mself\u001B[39m._general_file_writer = tf.io.gfile.GFile(\u001B[38;5;28mself\u001B[39m._file_name, \u001B[33m\"\u001B[39m\u001B[33mwb\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:513\u001B[39m, in \u001B[36mrecursive_create_dir_v2\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    501\u001B[39m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mio.gfile.makedirs\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    502\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mrecursive_create_dir_v2\u001B[39m(path):\n\u001B[32m    503\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Creates a directory and all parent/intermediate directories.\u001B[39;00m\n\u001B[32m    504\u001B[39m \n\u001B[32m    505\u001B[39m \u001B[33;03m  It succeeds if path already exists and is writable.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    511\u001B[39m \u001B[33;03m    errors.OpError: If the operation fails.\u001B[39;00m\n\u001B[32m    512\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m513\u001B[39m   \u001B[43m_pywrap_file_io\u001B[49m\u001B[43m.\u001B[49m\u001B[43mRecursivelyCreateDir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompat\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath_to_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFailedPreconditionError\u001B[39m: runs is not a directory"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "source": [
    "list(model.parameters())[1] #可学习的模型参数"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:06.448519700Z",
     "start_time": "2025-07-26T13:01:05.291821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-8.5894e-03, -3.4112e-02,  2.9327e-02, -2.9419e-02,  1.0954e-02,\n",
       "        -3.2812e-02,  3.1587e-02,  2.6264e-02, -3.5407e-02, -1.8975e-02,\n",
       "        -7.7493e-03, -3.2060e-02, -3.2450e-02,  1.9280e-03, -2.2649e-02,\n",
       "         2.2548e-02,  2.9107e-02,  8.8909e-04,  2.9437e-02, -1.7179e-02,\n",
       "        -2.3147e-02,  3.1192e-02, -1.8738e-02,  1.0304e-02, -1.5752e-02,\n",
       "        -2.9991e-02, -6.5195e-03,  1.9743e-03, -1.8758e-02, -2.6609e-02,\n",
       "        -1.6221e-02,  1.3980e-02,  4.1518e-03, -2.5361e-02, -8.8645e-03,\n",
       "        -2.6476e-02,  2.9848e-02, -2.6629e-02,  9.6866e-04, -6.0798e-03,\n",
       "        -3.5144e-02, -1.3698e-02, -3.1894e-02, -1.1581e-02,  3.4285e-02,\n",
       "        -7.1575e-03, -2.4025e-02, -1.0597e-02, -7.4010e-03, -2.5739e-02,\n",
       "         3.1889e-02,  3.8914e-03,  4.8110e-03, -2.6789e-02,  2.9049e-02,\n",
       "         4.6104e-03, -3.0268e-02, -1.8432e-02,  8.5847e-03,  1.6800e-02,\n",
       "         2.2064e-02,  3.2650e-02,  6.4975e-04, -2.7887e-02, -2.5038e-04,\n",
       "         3.1437e-02,  3.3987e-02,  3.3864e-02,  9.3410e-04,  6.8332e-04,\n",
       "        -2.3012e-02, -3.1535e-03,  8.7816e-03,  1.8281e-02, -1.5461e-02,\n",
       "        -2.7616e-02, -2.6675e-02, -2.1110e-02,  9.8585e-03, -6.0859e-03,\n",
       "        -2.2086e-02,  3.3985e-02, -1.9885e-03, -3.2009e-02, -2.0278e-02,\n",
       "        -2.1922e-02,  6.6384e-03,  1.6820e-02,  7.9774e-03,  1.1400e-02,\n",
       "        -9.7544e-03,  3.4115e-02,  9.0636e-03, -1.9163e-04,  5.5955e-03,\n",
       "         2.7037e-02, -3.3920e-03,  3.5572e-02, -1.2972e-02,  1.9067e-02,\n",
       "         2.4049e-02,  2.1050e-02,  1.8734e-02, -2.8215e-02,  2.6127e-02,\n",
       "        -1.2647e-02,  1.9820e-02,  1.2476e-02,  2.6762e-02,  8.7421e-03,\n",
       "         3.3159e-02, -3.0704e-02, -1.8541e-04, -1.3260e-03, -1.2233e-02,\n",
       "        -3.5064e-02,  3.3458e-02, -6.9668e-03,  3.2357e-02,  2.6385e-02,\n",
       "         1.5831e-02,  2.3978e-02, -3.2693e-02, -1.8065e-02,  3.0779e-02,\n",
       "         8.3476e-03, -4.8444e-03, -3.7081e-03,  4.6904e-03, -2.1728e-03,\n",
       "         3.4169e-02,  2.4811e-02, -6.7016e-03,  2.3581e-02, -2.9889e-02,\n",
       "         2.3142e-02, -4.9178e-03, -1.6103e-02,  7.5885e-03, -2.7685e-02,\n",
       "         2.2433e-02,  3.3838e-02, -2.4115e-03,  2.1302e-02,  1.1339e-03,\n",
       "        -8.4336e-03,  7.8512e-03,  3.3220e-02,  2.1302e-02, -3.0958e-02,\n",
       "        -2.0094e-02,  1.5889e-02,  2.8785e-03,  1.8595e-02,  1.9256e-02,\n",
       "         1.3932e-02, -2.9860e-02,  3.3363e-02,  2.2382e-02, -6.1448e-03,\n",
       "         3.2242e-02,  1.8734e-03, -2.6596e-02,  3.5623e-02,  1.4946e-02,\n",
       "        -2.3733e-02, -2.4101e-02,  1.5221e-02,  7.7907e-03, -2.9043e-02,\n",
       "        -1.0389e-02, -2.2812e-02,  1.6908e-03, -5.6758e-03, -2.6069e-03,\n",
       "         2.3556e-02, -3.3882e-02, -2.4385e-02,  1.8046e-02,  3.2529e-03,\n",
       "        -2.7668e-03,  1.1480e-02, -6.7090e-03,  1.9659e-03,  1.9628e-02,\n",
       "         1.5005e-02, -8.4782e-03,  2.3161e-02, -2.1535e-02,  1.2444e-02,\n",
       "        -2.0668e-02, -1.6889e-02,  3.5278e-02,  8.7268e-03,  1.4254e-03,\n",
       "         2.8698e-02, -2.2931e-02,  2.5828e-02, -1.1704e-02,  2.9392e-02,\n",
       "         3.1149e-02,  1.6547e-02, -5.9535e-03, -3.0256e-02,  1.3549e-02,\n",
       "         3.3213e-02,  1.1436e-03, -1.2671e-02, -3.0774e-02,  6.8064e-04,\n",
       "        -9.7905e-03,  5.4912e-03,  1.9277e-02, -3.0746e-02,  7.2306e-03,\n",
       "         3.5051e-03,  2.8134e-03, -2.8740e-02, -2.2817e-02, -2.9999e-03,\n",
       "         3.4408e-02, -1.8166e-02, -3.1768e-02, -3.2862e-02, -6.8035e-03,\n",
       "        -2.1402e-02, -2.0080e-02,  1.6999e-02,  2.7207e-03, -2.6727e-02,\n",
       "        -1.4606e-02, -1.1827e-02, -4.3236e-03, -3.2057e-02, -1.4067e-02,\n",
       "        -1.2665e-03,  5.8477e-03,  1.9040e-02,  3.4079e-02,  1.3302e-02,\n",
       "        -1.8123e-02, -3.4547e-02, -3.2043e-02,  2.4916e-02,  2.2325e-02,\n",
       "        -2.8183e-02,  1.0348e-02, -1.4458e-02, -2.9281e-02, -1.9515e-02,\n",
       "        -1.9547e-02,  1.5057e-02, -9.5960e-03,  1.7917e-02,  1.5139e-02,\n",
       "         1.1299e-02,  2.5971e-02,  1.9336e-03,  3.0798e-02,  2.3401e-02,\n",
       "         1.4090e-02,  1.4033e-02,  3.3868e-03,  2.7303e-05, -1.7078e-02,\n",
       "         2.0685e-03,  2.9687e-02, -2.0089e-02,  2.4500e-02, -3.2164e-02,\n",
       "         2.2669e-03,  2.2074e-02,  3.3281e-02,  2.0207e-02, -1.5303e-02,\n",
       "         4.3660e-03,  2.0309e-02, -2.2379e-02, -1.2767e-02,  1.8321e-02,\n",
       "        -1.9566e-02, -3.1332e-02, -3.4898e-02, -2.0204e-02, -3.1689e-02,\n",
       "         3.1479e-02, -7.4483e-03, -3.3350e-02,  3.2585e-02, -2.1639e-02,\n",
       "        -2.9836e-02, -2.2091e-02,  1.2804e-02,  2.1944e-02, -3.4749e-02,\n",
       "        -1.2622e-02,  1.8305e-02, -3.4085e-02,  1.4778e-02,  3.0295e-02],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "source": [
    "model.state_dict().keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:06.466471Z",
     "start_time": "2025-07-26T13:01:06.793132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias', 'linear_relu_stack.2.weight', 'linear_relu_stack.2.bias', 'linear_relu_stack.4.weight', 'linear_relu_stack.4.bias'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "source": [
    "model = model.to(device)\n",
    "record = training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    tensorboard_callback=tensorboard_callback,\n",
    "    save_ckpt_callback=save_ckpt_callback,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    eval_step=1000\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#帮我写个enumerate例子\n",
    "for i, item in enumerate([\"a\", \"b\", \"c\"]):\n",
    "    print(i, item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:06.501378400Z",
     "start_time": "2025-07-26T13:01:11.006769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:06.508359Z",
     "start_time": "2025-07-26T13:15:05.856485Z"
    }
   },
   "source": [
    "#画线要注意的是损失是不一定在零到1之间的\n",
    "def plot_learning_curves(record_dict, sample_step=500):\n",
    "    # build DataFrame\n",
    "    train_df = pd.DataFrame(record_dict[\"train\"]).set_index(\"step\").iloc[::sample_step]\n",
    "    val_df = pd.DataFrame(record_dict[\"val\"]).set_index(\"step\")\n",
    "\n",
    "    # plot\n",
    "    fig_num = len(train_df.columns) #因为有loss和acc两个指标，所以画个子图\n",
    "    fig, axs = plt.subplots(1, fig_num, figsize=(5 * fig_num, 5)) #fig_num个子图，figsize是子图大小\n",
    "    for idx, item in enumerate(train_df.columns):    \n",
    "        axs[idx].plot(train_df.index, train_df[item], label=f\"train_{item}\")\n",
    "        axs[idx].plot(val_df.index, val_df[item], label=f\"val_{item}\")\n",
    "        axs[idx].grid()\n",
    "        axs[idx].legend()\n",
    "        x_data=range(0, train_df.index[-1], 5000) #每隔5000步标出一个点\n",
    "        axs[idx].set_xticks(x_data)\n",
    "        axs[idx].set_xticklabels(map(lambda x: f\"{int(x/1000)}k\", x_data)) #map生成labal\n",
    "        axs[idx].set_xlabel(\"step\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(record, sample_step=500)  #横坐标是 steps"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[68]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     18\u001B[39m         axs[idx].set_xlabel(\u001B[33m\"\u001B[39m\u001B[33mstep\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     20\u001B[39m     plt.show()\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m plot_learning_curves(\u001B[43mrecord\u001B[49m, sample_step=\u001B[32m500\u001B[39m)  \u001B[38;5;66;03m#横坐标是 steps\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'record' is not defined"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = NeuralNetwork() #上线时加载模型\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:06.513346400Z",
     "start_time": "2025-07-26T13:00:15.294485Z"
    }
   },
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T19:30:06.514343200Z",
     "start_time": "2025-07-26T13:00:17.326703Z"
    }
   },
   "source": [
    "# dataload for evaluating\n",
    "\n",
    "# load checkpoints\n",
    "model.load_state_dict(torch.load(\"checkpoints/best.ckpt\", map_location=\"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "loss, acc = evaluating(model, val_loader, loss_fct)\n",
    "print(f\"loss:     {loss:.4f}\\naccuracy: {acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:     0.4264\n",
      "accuracy: 0.8471\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
