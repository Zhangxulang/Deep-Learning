# 优化器

### SGD

容易陷入鞍点

### AdaGrad

原理

优点

缺点

### RMSProp

原理

优点

缺点

### Adam

原理

优点

缺点





# w和b初始化

Xavier均匀分布

he分布



# 调哪些超参

优化器

激活函数

初始化

批归一化

数据增强

更多调参技巧   拿更多数据     添加神经元层次  看论文

学习率、梯度、损失是什么关系？

损失函数=目标函数







# fine—tune即精调/微调

![image-20250730173705634](C:\Users\oyaZXL\AppData\Roaming\Typora\typora-user-images\image-20250730173705634.png)



先对模型进行一个预训练，接着再次训练时，冻结一些层，重点训练一些层，从而达到更好的效果
