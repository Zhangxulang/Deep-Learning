![image-20250813022219219](C:\Users\oyaZXL\AppData\Roaming\Typora\typora-user-images\image-20250813022219219.png)









![image-20250813024007346](C:\Users\oyaZXL\AppData\Roaming\Typora\typora-user-images\image-20250813024007346.png)



这是 **多头注意力（Multi-Head Attention）** 的架构示意图，它基于你之前了解的 “缩放点积注意力（Scaled Dot-Product Attention）”，通过并行的多个注意力头，让模型从不同视角捕捉序列信息，下面拆解每个模块：

### 1. 底层 `Linear`（线性变换层）

- **输入**：接收原始的 Query（Q）、Key（K）、Value（V） 。这三者是模型处理前的序列表示，形状通常为 `[batch_size, seq_len, d_model]`（`batch_size` 是批次大小，`seq_len` 是序列长度，`d_model` 是模型维度 ）。
- **作用**：对 Q、K、V 分别做线性变换，将它们映射到不同的低维空间（为后续多头并行做准备 ）。每个线性变换的输出维度是 `[batch_size, seq_len, d_k]`（`d_k = d_model / h` ，`h` 是注意力头数量 ），通过独立的线性层，让不同头后续能关注序列的不同特征。

### 2. `Scaled Dot-Product Attention`（缩放点积注意力，共 h 个）

- **输入**：每个注意力头接收经过线性变换后的 Q、K、V （每个头的输入形状是 `[batch_size, seq_len, d_k]` ）。
- **作用**：如之前解释，每个头独立计算注意力，得到该头对序列的注意力权重和加权后的输出。h 个注意力头并行计算，每个头从不同 “视角” 捕捉序列间的关联（比如有的头关注语法结构，有的关注语义关联 ），输出形状为 `[batch_size, seq_len, d_k]` 。

### 3. `Concat`（拼接）

- **输入**：接收 h 个注意力头的输出（每个头输出形状 `[batch_size, seq_len, d_k]` ）。
- **作用**：把 h 个注意力头的输出在最后一维拼接起来，恢复到和原始输入类似的维度。拼接后形状变为 `[batch_size, seq_len, h * d_k]` ，而因为 `h * d_k = d_model`（`d_k = d_model / h` ），所以也可以说恢复到 `[batch_size, seq_len, d_model]` 维度，将多个头捕捉的不同信息整合在一起。

### 4. 顶层 `Linear`（线性变换层）

- **输入**：接收拼接后的特征（形状 `[batch_size, seq_len, d_model]` ）。
- **作用**：对拼接后的特征再做一次线性变换，将其映射回 `[batch_size, seq_len, d_model]` 维度（也可以根据需求调整输出维度，但通常保持和输入一致，方便后续残差连接 ）。这一步进一步融合多头的信息，让模型得到更丰富、更全面的序列表示。

### 整体流程总结

多头注意力通过 “线性变换拆分 → 多头并行计算注意力 → 拼接多头结果 → 线性变换融合” 的流程，让模型能从多个角度理解序列间的依赖关系，相比单头注意力，捕捉的信息更丰富、更全面，是 Transformer 模型强大表示能力的重要支撑 ，广泛应用于自然语言处理的各类任务（如翻译、文本生成、分类等 ）。



![image-20250813024141110](C:\Users\oyaZXL\AppData\Roaming\Typora\typora-user-images\image-20250813024141110.png)

![image-20250814032526174](C:\Users\oyaZXL\AppData\Roaming\Typora\typora-user-images\image-20250814032526174.png)

这是 **Transformer 模型的经典架构图**，展示了 Transformer 用于 seq2seq（序列到序列，如机器翻译）任务时的完整流程，包含 encoder（编码器）和 decoder（解码器）两大部分，下面分层拆解核心模块：

------

### **一、整体结构概览**

Transformer 由 **左侧 Encoder（编码器）** 和 **右侧 Decoder（解码器）** 组成：

- **Encoder**：负责编码输入序列（如源语言句子），提取全局特征表示。
- **Decoder**：负责根据编码器输出和解码器历史信息，生成目标序列（如目标语言句子）。
- **Nx**：表示模块重复堆叠的次数（论文中为 6 层），让模型逐层提炼特征。

### **二、Encoder（编码器）流程**

输入 → 词嵌入 + 位置编码 → 多层（Nx）编码块 → 输出编码特征

#### 1. **Input Embedding（输入嵌入）**

- **作用**：将输入的离散 token（如单词 ID）转换为连续向量（`d_model` 维度，论文中为 512），捕捉词的语义信息。
- **形状**：`[batch_size, seq_len, d_model]`

#### 2. **Positional Encoding（位置编码）**

- **作用**：Transformer 本身是 “位置无关” 的，通过位置编码注入序列的**顺序信息**（因为 Self-Attention 不区分 token 位置）。
- **实现**：用三角函数（正弦、余弦）生成位置向量，与词嵌入相加。
- **形状**：`[batch_size, seq_len, d_model]`

#### 3. **Multi-Head Attention（多头自注意力）**

- **输入**：编码块的输入（词嵌入 + 位置编码）同时作为 Q、K、V。
- **作用**：让每个位置的 token 关注序列中其他位置的信息，捕捉**全局依赖**（如 “猫” 和 “垫子” 的语义关联）。
- **输出**：融合多头部注意力的特征，形状 `[batch_size, seq_len, d_model]`

#### 4. **Add & Norm（残差连接 + 层归一化）**

- **残差连接（Add）**：`Output = Input + MultiHeadOutput`，缓解梯度消失，保留原始信息。
- **层归一化（Norm）**：对每个样本的每个层归一化（`LayerNorm`），稳定训练。

#### 5. **Feed Forward（前馈网络）**

- **结构**：两层线性变换 + ReLU 激活，公式：`FFN(x) = max(0, xW1 + b1)W2 + b2`
- **作用**：对每个 token 的特征独立做非线性变换，增强模型表达能力。
- **形状**：输入输出均为 `[batch_size, seq_len, d_model]`

#### 6. **Encoder 堆叠（Nx）**

将上述编码块（Multi-Head Attention + Add & Norm + Feed Forward + Add & Norm）重复堆叠 N 次（论文中 N=6），逐层提炼输入序列的特征表示。

### **三、Decoder（解码器）流程**

目标输入（移位后）→ 词嵌入 + 位置编码 → 多层（Nx）解码块 → 线性层 + Softmax → 输出概率

#### 1. **Output Embedding（输出嵌入）**

- **作用**：类似编码器的输入嵌入，将目标序列的离散 token 转换为连续向量。
- **细节**：输入会**右移（shifted right）**，防止解码器看到当前要预测的 token（如翻译时，预测第 i 个词不能依赖第 i 个词的输入）。

#### 2. **Positional Encoding（位置编码）**

- **作用**：与编码器一致，为目标序列注入位置信息。

#### 3. **Masked Multi-Head Attention（掩码多头自注意力）**

- **输入**：解码块的输入（词嵌入 + 位置编码）作为 Q、K、V。
- **掩码（Mask）**：遮挡当前位置右侧的 token（因为预测时无法看到未来信息），确保解码器 ** 自回归（autoregressive）** 生成。
- **作用**：让解码器关注目标序列的历史信息，逐步生成输出。

#### 4. **Multi-Head Attention（编码器 - 解码器注意力）**

- 输入

  ：

  - Q：解码器的中间特征（来自 Masked Attention 输出）。
  - K、V：编码器的最终输出（全局特征表示）。

- **作用**：让解码器关注编码器中与当前预测相关的信息（如翻译时，目标词对齐源语言的对应词）。

#### 5. **Add & Norm + Feed Forward**

- **与编码器一致**：残差连接 + 层归一化稳定训练，前馈网络增强特征表达。

#### 6. **输出层（Linear + Softmax）**

- **Linear**：将解码器输出（`d_model` 维度）映射到**词汇表大小（vocab_size）** 维度。
- **Softmax**：将线性层输出转换为概率分布，预测下一个 token 的概率。

### **四、核心设计思想**

1. **自注意力（Self-Attention）**：
   - 替代传统 RNN/CNN，高效捕捉长距离依赖（时间复杂度 *O*(*se**q*_*l**e**n*2) 但实际更灵活）。
   - 多头机制让模型从**多个视角**学习关联（如语法、语义、位置）。
2. **残差连接 + 层归一化**：
   - 解决深层网络训练难题（梯度消失、训练不稳定）。
3. **位置编码**：
   - 弥补自注意力 “位置无关” 的缺陷，让模型感知序列顺序。

### **五、典型应用**

- **机器翻译**：Encoder 编码源语言，Decoder 逐词生成目标语言。
- **文本生成**：如摘要、对话，Decoder 自回归生成序列。
- **分类任务**：只用 Encoder，取编码后的 `[CLS]` token 做分类。

这张图清晰展现了 Transformer 的 “编码 - 解码” 范式，是理解 BERT、GPT 等大模型的基础！如果需要深入某模块（如位置编码细节、注意力掩码），可以继续拆解～