##### 

ä»€ä¹ˆæ˜¯Tokenï¼Ÿå“ªé‡Œå¯ä»¥æŸ¥çœ‹ä¸€å¥è¯çš„tokenç»„æˆ

###### ChatGPT è¯´ï¼š

â€œTokenâ€ï¼ˆ**æ ‡è®°**ï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æœ€å°çš„è¯­è¨€å•ä½ï¼Œåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å«ä¹‰ç•¥æœ‰ä¸åŒï¼Œä½†åœ¨å¤§å¤šæ•°AIè¯­è¨€æ¨¡å‹ä¸­ï¼ŒToken æ˜¯ä¸€å¥è¯è¢«**åˆ‡åˆ†æˆæ¨¡å‹å¯ä»¥å¤„ç†çš„å•ä½**ã€‚

------

### âœ… ä¸€å¥è¯è§£é‡Šï¼š

> **Token æ˜¯è¯­è¨€æ¨¡å‹å¤„ç†æ–‡æœ¬çš„æœ€å°å•ä½ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªè¯ã€ä¸€éƒ¨åˆ†è¯ã€ä¸€ä¸ªç¬¦å·ï¼Œç”šè‡³ä¸€ä¸ªæ ‡ç‚¹ã€‚**

------

### ğŸ“Œ ä¸¾ä¸ªä¾‹å­ï¼š

å¥å­ï¼š

> ```
> ChatGPT is amazing!
> ```

è¢«åˆ‡åˆ†ä¸ºå¦‚ä¸‹ Tokenï¼ˆä»¥ OpenAI GPT-4 æ¨¡å‹ä¸ºä¾‹ï¼‰ï¼š

```
['Chat', 'G', 'PT', ' is', ' amazing', '!']
```

æ€»å…± 6 ä¸ª tokenã€‚

å†çœ‹ä¸­æ–‡ä¾‹å­ï¼š

> ```
> æˆ‘å–œæ¬¢å­¦ä¹ ã€‚
> ```

å¯èƒ½è¢«åˆ‡æˆï¼š

```
['æˆ‘', 'å–œæ¬¢', 'å­¦ä¹ ', 'ã€‚']
```

å…± 4 ä¸ª tokenã€‚

------

### ğŸ§  ä¸ºä»€ä¹ˆè¦ç”¨ Tokenï¼Ÿ

è¯­è¨€æ¨¡å‹ä¸æ˜¯ç›´æ¥ç†è§£æ•´ä¸ªå¥å­çš„ï¼Œè€Œæ˜¯æŠŠå¥å­åˆ†å‰²æˆ Tokenï¼Œå†å°†è¿™äº› Token ç¼–ç æˆå‘é‡è¾“å…¥ç¥ç»ç½‘ç»œè¿›è¡Œç†è§£å’Œç”Ÿæˆã€‚

------

### ğŸ” å¦‚ä½•æŸ¥çœ‹ä¸€å¥è¯çš„ Token ç»„æˆï¼Ÿ

#### âœ… æ–¹æ³•ä¸€ï¼šä½¿ç”¨ OpenAI å®˜æ–¹å·¥å…·ï¼ˆæ¨èï¼‰

- è®¿é—®ï¼šhttps://platform.openai.com/tokenizer
- è¾“å…¥ä½ çš„ä¸€å¥è¯ï¼Œå°±èƒ½çœ‹åˆ°å¯¹åº”çš„ Token åˆ‡åˆ†ç»“æœå’Œæ€»æ•°é‡ã€‚

#### âœ… æ–¹æ³•äºŒï¼šä½¿ç”¨ Python å·¥å…·åŒ…

å¦‚æœä½ ç”¨ Pythonï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `tiktoken`ï¼ˆOpenAI çš„ Tokenizerï¼‰ï¼š

```
import tiktoken

# ä»¥ GPT-4 æ¨¡å‹ä¸ºä¾‹
enc = tiktoken.encoding_for_model("gpt-4")

text = "ChatGPT is amazing!"
tokens = enc.encode(text)
print(tokens)             # è¾“å‡º token ID åºåˆ—
print(len(tokens))        # è¾“å‡º token æ•°é‡
```

------

### ğŸ“¦ è¡¥å……è¯´æ˜

- è‹±æ–‡ä¸­ï¼ŒToken é€šå¸¸æ˜¯ä¸€ä¸ªè¯æˆ–è€…è¯çš„ä¸€éƒ¨åˆ†ï¼ˆæ¯”å¦‚ `ChatGPT` ä¼šè¢«æ‹†å¼€ï¼‰ã€‚
- ä¸­æ–‡ä¸­ï¼ŒToken é€šå¸¸æ˜¯ä¸€ä¸ªæ±‰å­—æˆ–ä¸€ä¸ªè¯ï¼ˆå–å†³äºæ¨¡å‹å’Œ Tokenizer ç®—æ³•ï¼‰ã€‚
- GPT æ¨¡å‹ä¸­ï¼š**1 ä¸ª token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯**æˆ–**1-2 ä¸ªæ±‰å­—**ã€‚

------

å¦‚æœä½ æœ‰å…·ä½“çš„ä¸€å¥è¯ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç«‹å³æŸ¥çœ‹å®ƒçš„ token åˆ‡åˆ†å’Œ token æ•°é‡ã€‚æ¬¢è¿å‘è¿‡æ¥æµ‹è¯•ã€‚

##### æ‚¨è¯´ï¼š

tokençš„idæ˜¯å›ºå®šçš„å—ï¼Œæœ‰å¤šå°‘ä¸ª

###### ChatGPT è¯´ï¼š

æ˜¯çš„ï¼Œ**Token çš„ ID æ˜¯å›ºå®šçš„**ï¼Œæ¯ä¸€ä¸ª Token åœ¨æ¨¡å‹ä¸­éƒ½å¯¹åº”ç€ä¸€ä¸ªå”¯ä¸€çš„ **Token ID**ï¼Œè¿™äº› ID å’Œ Token æ˜¯é€šè¿‡ä¸€ä¸ªå­—å…¸ï¼ˆ**è¯è¡¨ / vocabulary**ï¼‰ä¸€ä¸€æ˜ å°„çš„ã€‚è¿™ä¸ªè¯è¡¨åœ¨æ¨¡å‹è®­ç»ƒæ—¶å°±å›ºå®šä¸‹æ¥äº†ï¼ŒåæœŸä¸ä¼šæ”¹å˜ã€‚

------

### âœ… ä¸€å¥è¯æ€»ç»“ï¼š

> **æ¯ä¸€ä¸ª Tokenï¼ˆå¦‚ "Hello"ã€"ä½ "ã€"ã€‚"ï¼‰éƒ½æ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šçš„ Token IDï¼Œæ¨¡å‹è¯†åˆ«çš„æ˜¯è¿™äº› IDã€‚**

------

### ğŸ”¢ æœ‰å¤šå°‘ä¸ª Tokenï¼Ÿ

è¿™å–å†³äºä½ ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸åŒæ¨¡å‹çš„è¯è¡¨å¤§å°ä¸åŒï¼š

| æ¨¡å‹åç§°        | è¯è¡¨å¤§å°ï¼ˆToken æ€»æ•°ï¼‰ | Tokenizer ç±»å‹                    |
| --------------- | ---------------------- | --------------------------------- |
| GPT-2           | 50,257                 | Byte Pair Encoding (BPE)          |
| GPT-3 / GPT-3.5 | 50,257                 | BPEï¼ˆä¸GPT-2ç›¸åŒï¼‰                |
| GPT-4           | 100,000+ï¼ˆå…·ä½“æœªå…¬å¼€ï¼‰ | tiktoken + å¤šè¯­è¨€æ”¯æŒï¼ˆæ›´å¤§è¯è¡¨ï¼‰ |
| Claude 2/3      | ~200,000+ï¼ˆä¼°è®¡ï¼‰      | SentencePiece / BPE               |
| LLaMA / Alpaca  | ~32,000                | SentencePiece                     |

------

### ğŸ“˜ ç¤ºä¾‹ï¼šToken å’Œ ID çš„å¯¹åº”å…³ç³»

ä»¥ GPT-2 ä¸ºä¾‹ï¼š

```
import tiktoken

enc = tiktoken.get_encoding("gpt2")
text = "Hello, world!"
tokens = enc.encode(text)
print(tokens)  # è¾“å‡ºæ¯ä¸ª token çš„ ID
print([enc.decode([t]) for t in tokens])  # è¾“å‡ºå¯¹åº”çš„ token å†…å®¹
```

è¾“å‡ºç±»ä¼¼äºï¼š

```
[15496, 11, 995]           # æ¯ä¸ªæ•°å­—æ˜¯ä¸€ä¸ª token ID
['Hello', ',', ' world']  # æ¯ä¸ª token å¯¹åº”çš„æ–‡æœ¬
```

------

### ğŸ§  ä¸ºä»€ä¹ˆå›ºå®šï¼Ÿ

- æ¨¡å‹è®­ç»ƒæ—¶å°±å»ºç«‹å¥½äº†è¿™ä¸ªè¯å…¸ï¼Œæ¯ä¸€ä¸ª token éƒ½å˜æˆä¸€ä¸ªå›ºå®šçš„å‘é‡ IDã€‚
- æ¨¡å‹å­¦ä¹ çš„æ˜¯è¿™äº› ID ä¸è¯­ä¹‰ä¹‹é—´çš„å…³ç³»ï¼Œä¸èƒ½åœ¨è¿è¡Œæ—¶æ”¹å˜ ID æˆ–è¯è¡¨ã€‚

------

### âœ… æ€»ç»“ï¼š

| é—®é¢˜                     | å›ç­”                                                 |
| ------------------------ | ---------------------------------------------------- |
| Token çš„ ID æ˜¯å›ºå®šçš„å—ï¼Ÿ | âœ… æ˜¯çš„ï¼Œæ¯ä¸ª Token åœ¨è¯è¡¨ä¸­æœ‰å”¯ä¸€ IDã€‚               |
| Token æœ‰å¤šå°‘ä¸ªï¼Ÿ         | â—è§†æ¨¡å‹è€Œå®šï¼Œæ¯”å¦‚ GPT-2 æ˜¯ 50,257 ä¸ªã€‚               |
| å¯ä»¥è‡ªå®šä¹‰ Token å—ï¼Ÿ    | âŒ ä¸å¯ä»¥ï¼Œé™¤éä½ é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–ä½¿ç”¨è‡ªå®šä¹‰ tokenizerã€‚ |

------

å¦‚æœä½ éœ€è¦æŸ¥çœ‹æŸæ®µæ–‡æœ¬çš„ token å’Œå¯¹åº”çš„ IDï¼Œæˆ‘å¯ä»¥ç›´æ¥å¸®ä½ ç”Ÿæˆï¼Œéšæ—¶å‘æˆ‘å†…å®¹å°±å¯ä»¥ã€‚

##### æ‚¨è¯´ï¼š

ä¸åŒæ¨¡å‹çš„è¯å…¸tokenå’Œidæ˜ å°„å…³ç³»ä¸€æ ·å—

###### ChatGPT è¯´ï¼š

ä¸åŒæ¨¡å‹çš„è¯å…¸ï¼ˆToken â†’ ID çš„æ˜ å°„ï¼‰**é€šå¸¸æ˜¯ä¸ä¸€æ ·çš„**ã€‚ä¸‹é¢æ˜¯è¯¦ç»†è§£é‡Šï¼š

------

## âœ… æ€»ç»“å›ç­”ï¼š

> **ä¸åŒæ¨¡å‹çš„è¯å…¸ï¼ˆå³ Token ä¸ ID çš„æ˜ å°„å…³ç³»ï¼‰ä¸€èˆ¬éƒ½æ˜¯ä¸åŒçš„ã€‚**
> å³ä½¿ä¸¤ä¸ªæ¨¡å‹å¤„ç†åŒä¸€å¥è¯ï¼Œå…¶åˆ‡åˆ†å‡ºçš„ Token å’Œæ¯ä¸ª Token å¯¹åº”çš„ ID å¾€å¾€ä¹Ÿä¸ä¸€æ ·ã€‚

------

## ğŸ” ä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿ

1. **æ¨¡å‹ä½¿ç”¨çš„ Tokenizer ä¸åŒ**
   - GPT-2ã€GPT-3 ä½¿ç”¨çš„æ˜¯ OpenAI è‡ªå®¶çš„ BPEï¼ˆByte-Pair Encodingï¼‰ã€‚
   - GPT-4 ä½¿ç”¨äº†æ›´å¤æ‚ã€æ›´å¤§çš„å¤šè¯­è¨€è¯è¡¨ï¼Œæ”¹è¿›äº† tiktoken ç¼–ç æ–¹å¼ã€‚
   - LLaMAã€T5ã€BERT ä½¿ç”¨ SentencePieceï¼Œé€‚åˆå¤šè¯­è¨€æˆ–å­è¯ç²’åº¦ç¼–ç ã€‚
   - Claude ä½¿ç”¨ Anthropic è‡ªç ”çš„ tokenizerï¼Œæ˜ å°„æ–¹å¼ä¸å…¬å¼€ã€‚
2. **è¯è¡¨å¤§å°ä¸åŒ**
   - GPT-2ï¼š50,257 ä¸ª Token
   - LLaMAï¼š32,000 ä¸ª Token
   - GPT-4ï¼šå®˜æ–¹æœªå…¬å¼€ï¼Œä½†æ¯” GPT-3 å¤§å¾ˆå¤š
3. **ä¸åŒæ¨¡å‹å¯¹ç©ºæ ¼ã€æ ‡ç‚¹ã€ä¸­æ–‡ç­‰çš„å¤„ç†æ–¹å¼ä¸åŒ**
   - æ¯”å¦‚ GPT ç³»åˆ—ç»å¸¸æŠŠ `" GPT"`ï¼ˆæ³¨æ„å‰é¢ç©ºæ ¼ï¼‰ä½œä¸ºä¸€ä¸ª Tokenã€‚
   - è€Œ LLaMA æ›´å€¾å‘äºæŒ‰å•å­—ã€å­è¯æˆ–å®Œæ•´è¯è¿›è¡Œåˆ‡åˆ†ã€‚

------

## ğŸ“Œ ä¸¾ä¸ªå¯¹æ¯”ä¾‹å­

å¥å­ï¼š`ä½ å¥½ï¼Œä¸–ç•Œï¼`

| æ¨¡å‹  | Token åˆ‡åˆ†ç¤ºä¾‹                       | Token ID ç¤ºä¾‹                            |
| ----- | ------------------------------------ | ---------------------------------------- |
| GPT-3 | ['ä½ ', 'å¥½', 'ï¼Œ', 'ä¸–ç•Œ', 'ï¼']     | [12345, 54321, 2001, 8765, 3344]ï¼ˆä¸¾ä¾‹ï¼‰ |
| LLaMA | ['â–ä½ å¥½', 'ï¼Œ', 'ä¸–', 'ç•Œ', 'ï¼']    | [2354, 12, 561, 768, 33]ï¼ˆä¸¾ä¾‹ï¼‰         |
| BERT  | ['ä½ ', 'å¥½', 'ï¼Œ', 'ä¸–', 'ç•Œ', 'ï¼'] | [1379, 2088, 8024, 2466, 2434, 8013]     |

å¯ä»¥çœ‹åˆ°ï¼š

- æ¯ä¸ªæ¨¡å‹åˆ‡åˆ†æ–¹å¼ä¸åŒï¼ˆæœ‰çš„åˆå¹¶æˆè¯ï¼Œæœ‰çš„æ‹†åˆ†ä¸ºå­—ï¼‰
- æ¯ä¸ª Token çš„ ID å®Œå…¨ä¸ä¸€æ ·

------

## ğŸ’¡ å¦‚ä½•æŸ¥çœ‹æŸä¸ªæ¨¡å‹çš„ Token-ID æ˜ å°„ï¼Ÿ

### âœ… æ–¹æ³•ä¸€ï¼šOpenAI æ¨¡å‹ï¼ˆå¦‚ GPT-3 / GPT-4ï¼‰

ç”¨ `tiktoken`ï¼š

```
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")
tokens = enc.encode("ä½ å¥½ï¼Œä¸–ç•Œï¼")
print(tokens)
print([enc.decode([t]) for t in tokens])
```

------

### âœ… æ–¹æ³•äºŒï¼šHugging Face æ¨¡å‹ï¼ˆå¦‚ BERT / LLaMAï¼‰

```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
tokens = tokenizer.tokenize("ä½ å¥½ï¼Œä¸–ç•Œï¼")
ids = tokenizer.convert_tokens_to_ids(tokens)
print(list(zip(tokens, ids)))
```

------

## âœ… æ€»ç»“è¡¨æ ¼

| é¡¹ç›®                         | æ˜¯å¦ç›¸åŒ                          |
| ---------------------------- | --------------------------------- |
| Token åˆ‡åˆ†æ–¹å¼               | âŒ ä¸åŒï¼ˆæ¯ä¸ªæ¨¡å‹å®šä¹‰ä¸åŒï¼‰        |
| Token â†’ ID æ˜ å°„å…³ç³»          | âŒ ä¸åŒï¼ˆè¯è¡¨ã€ID å®Œå…¨ä¸åŒï¼‰       |
| åŒä¸€ä¸ª Token åœ¨ä¸åŒæ¨¡å‹çš„ ID | âŒ ä¸åŒï¼ˆå¦‚ "ä½ " å¯èƒ½å¯¹åº”ä¸åŒ IDï¼‰ |

------

å¦‚ä½ æœ‰ç‰¹å®šæ¨¡å‹åç§°æˆ–ä¾‹å¥ï¼Œæˆ‘å¯ä»¥ç›´æ¥ç»™å‡ºè¯¥æ¨¡å‹çš„ token-id æ˜ å°„å…³ç³»ã€‚è¦ä¸è¦æˆ‘å¸®ä½ æ¼”ç¤ºï¼Ÿ

##### æ‚¨è¯´ï¼š

è¯¦ç»†ä»‹ç»Byte Pair Encoding (BPE)

###### ChatGPT è¯´ï¼š

å½“ç„¶å¯ä»¥ï¼Byte Pair Encodingï¼ˆç®€ç§° **BPE**ï¼‰æ˜¯ä¸€ç§éå¸¸å¸¸è§çš„ **å­è¯åˆ†è¯ç®—æ³•ï¼ˆsubword tokenizationï¼‰**ï¼Œå¹¿æ³›ç”¨äºç°ä»£ NLP æ¨¡å‹ä¸­ï¼Œæ¯”å¦‚ GPTã€RoBERTaã€OpenAI Codex ç­‰ã€‚å®ƒçš„ç›®æ ‡æ˜¯åœ¨è¯æ±‡é‡å¤§å°å’Œè¡¨ç¤ºèƒ½åŠ›ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

------

## ğŸ§  ä¸€å¥è¯è§£é‡Šï¼š

> **BPE æ˜¯ä¸€ç§å°†è¯åˆ†è§£ä¸ºé¢‘ç‡æ›´é«˜çš„å­è¯ï¼ˆSubword Unitsï¼‰çš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹æ—¢èƒ½å¤„ç†å¸¸è§è¯ï¼Œä¹Ÿèƒ½å¤„ç†ç½•è§è¯æˆ–æ–°è¯ã€‚**

------

## ğŸ”§ BPE çš„åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

åœ¨ NLP ä¸­ä½ å¯èƒ½é‡åˆ°ä¸¤ä¸ªæç«¯çš„åˆ†è¯æ–¹å¼ï¼š

- **è¯çº§åˆ†è¯ï¼ˆword-levelï¼‰**ï¼šç®€å•ï¼Œä½†æ— æ³•å¤„ç† OOVï¼ˆæœªçŸ¥è¯ï¼ŒOut-of-Vocabularyï¼‰ã€‚
  - æ¯”å¦‚ä½ è®­ç»ƒæ—¶æ²¡è§è¿‡ "ChatGPT"ï¼Œæ¨¡å‹æ ¹æœ¬å¤„ç†ä¸äº†ã€‚
- **å­—ç¬¦çº§åˆ†è¯ï¼ˆchar-levelï¼‰**ï¼šå¯ä»¥å¤„ç†æ‰€æœ‰è¯ï¼Œä½†ç²’åº¦å¤ªç»†ï¼Œå¥å­å˜å¾—å¾ˆé•¿ï¼Œè®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä½ã€‚

æ‰€ä»¥éœ€è¦ **å­è¯åˆ†è¯ï¼ˆsubword-levelï¼‰**ï¼Œå®ƒï¼š
âœ… èƒ½å¤„ç†æœªçŸ¥è¯
âœ… èƒ½ä¿æŒåˆç†çš„åºåˆ—é•¿åº¦
âœ… è¯è¡¨å¤§å°å¯æ§

------

## ğŸ”„ BPE çš„æ ¸å¿ƒæ€æƒ³ï¼š

### > **é¢‘ç¹çš„å­—ç¬¦å¯¹åˆå¹¶**

é€šè¿‡ä¸æ–­åœ°å°†å‡ºç°é¢‘ç‡æœ€é«˜çš„ã€Œå­—ç¬¦å¯¹ã€åˆå¹¶æˆæ–°â€œç¬¦å·â€ï¼Œæœ€ç»ˆæ„å»ºä¸€ä¸ªåŒ…å«å¸¸è§è¯æ ¹ã€è¯ç¼€ã€å­è¯çš„è¯è¡¨ã€‚

------

## ğŸ“Œ BPE ç®—æ³•æ­¥éª¤ï¼ˆæ‰‹æŠŠæ‰‹ä¸¾ä¾‹ï¼‰

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå°å‹è¯­æ–™åº“ï¼š

```
low
lower
newer
newest
```

æˆ‘ä»¬å…ˆå°†æ‰€æœ‰è¯æ‹†æˆå­—ç¬¦å¹¶åŠ ä¸Šç»“æŸç¬¦ `</w>`ï¼ˆç¡®ä¿è¯å°¾ä¸ä¼šè¢«åˆå¹¶ï¼‰ï¼š

```
l o w </w>
l o w e r </w>
n e w e r </w>
n e w e s t </w>
```

ç„¶åæˆ‘ä»¬è¿›è¡Œå¦‚ä¸‹è¿­ä»£æ­¥éª¤ï¼š

------

### ğŸ” ç¬¬ 1 æ­¥ï¼šç»Ÿè®¡æ‰€æœ‰å­—ç¬¦å¯¹å‡ºç°é¢‘ç‡

| å­—ç¬¦å¯¹ï¼ˆbigramï¼‰ | é¢‘ç‡ |
| ---------------- | ---- |
| l o              | 2    |
| o w              | 2    |
| w </w>           | 1    |
| w e              | 2    |
| e r              | 2    |
| e s              | 1    |
| s t              | 1    |

------

### ğŸ” ç¬¬ 2 æ­¥ï¼šæ‰¾å‡ºå‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼Œåˆå¹¶æˆæ–°ç¬¦å·

æ¯”å¦‚ `e r` å‡ºç°æœ€å¤š â†’ åˆå¹¶ä¸º `er`ï¼š

æ›´æ–°è¯ï¼š

```
l o w </w>
l o w er </w>
n e w er </w>
n e w e s t </w>
```

ç»§ç»­ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡ï¼Œé‡å¤è¿™ä¸ªè¿‡ç¨‹ N æ¬¡ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾è¯è¡¨å¤§å°æˆ–åœæ­¢æ¡ä»¶ã€‚

------

### æœ€ç»ˆæ•ˆæœï¼š

- å¸¸è§è¯ï¼ˆå¦‚ â€œlowerâ€ï¼‰å¯èƒ½ä½œä¸ºæ•´ä½“æˆ–å¤§éƒ¨åˆ†è¢«åˆå¹¶ä¸ºä¸€ä¸ª tokenã€‚
- ç½•è§è¯ï¼ˆå¦‚ â€œunbelievabilityâ€ï¼‰ä¼šè¢«æ‹†åˆ†æˆï¼š`un`, `believe`, `abil`, `ity`

------

## ğŸ§± BPE çš„æœ€ç»ˆäº§ç‰©ï¼š

- ä¸€ä¸ªåŒ…å«è‹¥å¹²å­è¯çš„è¯è¡¨ï¼ˆtoken vocabularyï¼‰
- ä¸€ä¸ªå°†è¯æ‹†æˆè¿™äº›å­è¯çš„ç¼–ç å™¨ï¼ˆtokenizerï¼‰
- ä¸€ä¸ªå°† token ID è¿˜åŸä¸ºæ–‡æœ¬çš„è§£ç å™¨ï¼ˆdetokenizerï¼‰

------

## âœ… BPE çš„ä¼˜ç‚¹ï¼š

| ä¼˜ç‚¹               | æè¿°                                   |
| ------------------ | -------------------------------------- |
| âœ… å¯å¤„ç† OOV       | æ‰€æœ‰è¯éƒ½å¯ä»¥æ‹†æˆå­—ç¬¦ç»„åˆæ¥è¡¨ç¤º         |
| âœ… ä¿ç•™å¸¸ç”¨è¯æ•´ä½“æ€§ | é«˜é¢‘è¯å¸¸å¸¸è¢«åˆå¹¶ä¸ºæ•´ä½“ tokenï¼Œæå‡æ•ˆç‡ |
| âœ… è¯è¡¨å¤§å°å¯æ§     | ä½ å¯ä»¥é€‰ 10Kã€30Kã€50K token           |
| âœ… å…¼å®¹å¤šè¯­è¨€       | ç‰¹åˆ«é€‚åˆæ‹¼éŸ³æ–‡å­—æˆ–ä¸­æ—¥éŸ©è¯æ±‡ç¨€ç–è¯­è¨€   |

------

## âš ï¸ BPE çš„ç¼ºç‚¹ï¼š

| ç¼ºç‚¹                   | æè¿°                             |
| ---------------------- | -------------------------------- |
| âŒ ä¸ä¸€å®šç¬¦åˆè¯­è¨€å­¦åˆ’åˆ† | æœ‰äº›å­è¯æ‹†åˆ†å¯èƒ½ä¸ç¬¦åˆè¯æ³•è§„å¾‹   |
| âŒ åˆ†è¯ç»“æœä¸å¯é€†       | åˆå¹¶è¿‡ç¨‹æœ‰ä¸€å®šè´ªå¿ƒæ€§ï¼Œç»“æœä¸å”¯ä¸€ |
| âŒ ä¸é€‚ç”¨äºæç«¯é•¿è¯åˆ†è§£ | æŸäº›è¯­è¨€ä¸­ä¼šé€ æˆ token åºåˆ—çˆ†ç‚¸  |

------

## ğŸ” ä¸å…¶ä»–ç®—æ³•çš„å¯¹æ¯”ï¼š

| åˆ†è¯ç®—æ³•      | ç²’åº¦     | æ”¯æŒ OOV | è¯è¡¨å¤§å° | å¸¸ç”¨äº                |
| ------------- | -------- | -------- | -------- | --------------------- |
| Word-level    | ç²—       | âŒ        | å¤§       | æ—©æœŸ NLP              |
| Char-level    | ç»†       | âœ…        | å°       | RNNã€CNN              |
| **BPE**       | ä¸­       | âœ…        | ä¸­ç­‰     | GPT-2ã€RoBERTaã€GPT-3 |
| WordPiece     | ç±»ä¼¼ BPE | âœ…        | ä¸­ç­‰     | BERT                  |
| SentencePiece | æ›´æ³›åŒ–   | âœ…        | å¯è°ƒ     | T5ã€LLaMAã€BART       |

------

## ğŸ§ª ç¤ºä¾‹ä»£ç ï¼šè®­ç»ƒè‡ªå·±çš„ BPE Tokenizer

```
from tokenizers import ByteLevelBPETokenizer

# åˆå§‹åŒ– tokenizer
tokenizer = ByteLevelBPETokenizer()

# è®­ç»ƒ
tokenizer.train(files=["your_corpus.txt"], vocab_size=10000, min_frequency=2)

# ä¿å­˜
tokenizer.save_model("bpe_tokenizer")

# ä½¿ç”¨
encoded = tokenizer.encode("ChatGPT is powerful!")
print(encoded.tokens)
print(encoded.ids)
```

------

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> BPE æ˜¯ç°ä»£ NLP ä¸­æœ€å®ç”¨çš„å­è¯åˆ†è¯ç®—æ³•ä¹‹ä¸€ï¼Œèƒ½æœ‰æ•ˆå¤„ç†æœªç™»å½•è¯ï¼ŒåŒæ—¶å…¼é¡¾æ•ˆç‡ä¸è¡¨ç°åŠ›ã€‚

------

å¦‚æœä½ æƒ³äº†è§£ BPE åœ¨ GPT-2/GPT-3 ä¸­çš„å…·ä½“è¡¨ç°ï¼Œæˆ–è€…å®ƒçš„è¯è¡¨ç»“æ„ï¼Œæˆ‘å¯ä»¥ç»§ç»­ç»™å‡ºæ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚ï¼Œç”šè‡³å±•ç¤ºè¯è¡¨æ–‡ä»¶ã€‚éœ€è¦æˆ‘æ¼”ç¤ºä¸€ä¸‹ BPE åˆ†è¯è¿‡ç¨‹çš„ä»£ç æˆ–è¯è¡¨å¯è§†åŒ–å—ï¼Ÿ

##### æ‚¨è¯´ï¼š

vocab_size=10000,æ˜¯ä»€ä¹ˆæ„æ€

###### ChatGPT è¯´ï¼š

å¾ˆå¥½ï¼Œè¿™ä¸ªé—®é¢˜éå¸¸å…³é”®ã€‚ä½ çœ‹åˆ°çš„å‚æ•° `vocab_size=10000` å‡ºç°åœ¨è®­ç»ƒ BPE åˆ†è¯å™¨æ—¶ï¼Œæ„æ€å¦‚ä¸‹ï¼š

------

## âœ… ç®€å•è§£é‡Šï¼š

> `vocab_size=10000` è¡¨ç¤ºè¦ç”Ÿæˆä¸€ä¸ª **åŒ…å«æœ€å¤š 10,000 ä¸ª Token çš„è¯è¡¨**ï¼ˆvocabularyï¼‰ã€‚

------

## ğŸ“˜ æ›´è¯¦ç»†ä¸€ç‚¹ï¼š

è®­ç»ƒ BPE åˆ†è¯å™¨æ—¶ï¼Œæˆ‘ä»¬è¦æŒ‡å®šè¯è¡¨ï¼ˆtoken vocabularyï¼‰çš„æœ€å¤§å®¹é‡ï¼Œä¹Ÿå°±æ˜¯ **æœ€å¤šä¿ç•™å¤šå°‘ä¸ª Tokenï¼ˆå­è¯ã€å­—ç¬¦ã€è¯ï¼‰**ã€‚

- è¿™ä¸ªè¯è¡¨åŒ…å«ï¼š
  - æ‰€æœ‰æœ€ç»ˆåˆå¹¶å¾—åˆ°çš„ **å­è¯ï¼ˆsubwordsï¼‰**
  - ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚ `[PAD]`, `[UNK]`, `<s>`, `</s>` ç­‰ï¼‰

æ¯”å¦‚ä½ æŒ‡å®šäº† `vocab_size=10000`ï¼š

- ç®—æ³•æœ€å¤šä¿ç•™ **10,000 ä¸ª token å•å…ƒ**
- åŒ…æ‹¬ï¼š
  - é«˜é¢‘è¯ï¼Œå¦‚ï¼š`the`, `to`, `is`, `ä½ `, `æˆ‘`, `å­¦ä¹ `
  - é«˜é¢‘å­è¯ï¼Œå¦‚ï¼š`##ing`, `##tion`, `å­¦`, `ä¹ `
  - å­—ç¬¦ï¼Œå¦‚ï¼š`a`, `b`, `c`ï¼ˆä½é¢‘è¯æ‹†åˆ†åä¼šå›é€€åˆ°å­—ç¬¦ï¼‰

------

## ğŸ“Œ ä¸¾ä¸ªä¾‹å­

ä½ æœ‰ä¸€ä¸ªä¸­æ–‡+è‹±æ–‡çš„è¯­æ–™åº“ï¼Œè®­ç»ƒä¸€ä¸ª BPE åˆ†è¯å™¨ï¼š

```
tokenizer.train(
    files=["corpus.txt"],
    vocab_size=10000,
    min_frequency=2
)
```

è®­ç»ƒç»“æœï¼š

- è¯è¡¨ä¸­å¯èƒ½åŒ…å«ï¼š
  - è‹±æ–‡ token: `the`, `an`, `super`, `##vision`
  - ä¸­æ–‡ token: `ä½ `, `å­¦ä¹ `, `æ¨¡å‹`, `åŒ–`, `æ•°æ®`
  - æ ‡ç‚¹ç¬¦å·: `,`, `.`, `ï¼Ÿ`, `ï¼`
  - ç‰¹æ®Šç¬¦å·: `[PAD]`, `[UNK]`

æœ€ç»ˆï¼Œè¿™ 10,000 ä¸ª token ä¼šè¢«ç¼–ç ä¸º ID ä» `0` åˆ° `9999`ã€‚

------

## âš ï¸ è®¾ç½®è¿‡å¤§ or è¿‡å°ï¼Œä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

| vocab_size               | å½±å“                                         |
| ------------------------ | -------------------------------------------- |
| å¤ªå°ï¼ˆå¦‚ 500ï¼‰           | å­è¯æ‹†å¾—å¤ªç¢ï¼Œå¥å­è¢«ç¼–ç æˆå¤ªå¤š Tokenï¼Œæ•ˆç‡ä½ |
| å¤ªå¤§ï¼ˆå¦‚ 100,000ï¼‰       | æ¨¡å‹è§„æ¨¡å¢å¤§ï¼Œå ç”¨å†…å­˜å¤šï¼Œæ³›åŒ–èƒ½åŠ›ä¸‹é™       |
| åˆé€‚ï¼ˆå¦‚ 10,000~50,000ï¼‰ | é€šå¸¸èƒ½å–å¾—è¾ƒå¥½çš„åˆ†è¯æ•ˆæœå’Œè®¡ç®—æ•ˆç‡å¹³è¡¡       |

ä¸åŒæ¨¡å‹é€‰æ‹©çš„ vocab_sizeï¼š

- GPT-2ï¼š50,257
- BERTï¼ˆè‹±æ–‡ï¼‰ï¼š30,000
- LLaMAï¼š32,000
- å°å‹é¡¹ç›®å»ºè®®ï¼š10,000~30,000

------

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> `vocab_size=10000` è¡¨ç¤ºè®­ç»ƒå‡ºæ¥çš„è¯è¡¨ä¸­æœ€å¤šåªä¿ç•™ 10,000 ä¸ª Tokenï¼ˆåŒ…æ‹¬è¯ã€å­è¯ã€å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·ï¼‰ï¼Œè¿™äº› Token æ˜¯æ¨¡å‹ç†è§£å’Œç”Ÿæˆæ–‡æœ¬çš„æœ€å°å•ä½ã€‚

------

å¦‚æœä½ æœ‰ä¸€ä¸ªè¯­æ–™åº“ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”¨ `tokenizers` è®­ç»ƒä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œå¹¶å±•ç¤ºè¯è¡¨ä¸­å‰ N ä¸ª token æ˜¯ä»€ä¹ˆï¼Œè¦ä¸è¦è¯•è¯•ï¼Ÿ

##### æ‚¨è¯´ï¼š

é¢„æ–™åº“å¦‚ä¸‹ï¼šzwei junge weiÃŸe mÃ¤nner sind im freien in der nÃ¤he vieler bÃ¼sche . mehrere mÃ¤nner mit schutzhelmen bedienen ein antriebsradsystem . ein kleines mÃ¤dchen klettert in ein spielhaus aus holz . ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster . zwei mÃ¤nner stehen am herd und bereiten essen zu . ein mann in grÃ¼n hÃ¤lt eine gitarre , wÃ¤hrend der andere mann sein hemd ansieht . ein mann lÃ¤chelt einen ausgestopften lÃ¶wen an . ein schickes mÃ¤dchen spricht mit dem handy wÃ¤hrend sie langsam die straÃŸe entlangschwebt . eine frau mit einer groÃŸen geldbÃ¶rse geht an einem tor vorbei . jungen tanzen mitten in der nacht auf pfosten .

###### ChatGPT è¯´ï¼š

å¾ˆå¥½ï¼ä½ ç»™çš„è¿™æ®µè¯­æ–™æ˜¯å¾·è¯­å¥å­ç»„æˆçš„ä¸€ä¸ªå°å‹è¯­æ–™åº“ï¼Œéå¸¸é€‚åˆç”¨äºæ¼”ç¤º **Byte Pair Encoding (BPE)** åˆ†è¯è®­ç»ƒï¼Œå¹¶è¯´æ˜ `vocab_size=10000` çš„æ•ˆæœã€‚

æˆ‘ä»¬å°†ç”¨ `tokenizers`ï¼ˆæ¥è‡ª Hugging Face çš„åº“ï¼‰æ¥ï¼š

1. **è®­ç»ƒ BPE åˆ†è¯å™¨**
2. **æŸ¥çœ‹è¯è¡¨ä¸­çš„ tokenï¼ˆå­è¯ï¼‰**
3. **å¯¹ä¸€ä¸ªå¥å­è¿›è¡Œåˆ†è¯å¹¶è¾“å‡º token + ID**

------

## ğŸ§ª æ­¥éª¤ä¸€ï¼šåˆ›å»ºä¸´æ—¶æ–‡æœ¬æ–‡ä»¶ï¼ˆä½ çš„è¯­æ–™ï¼‰

å…ˆæŠŠè¿™æ®µæ–‡æœ¬ä¿å­˜ä¸º `corpus.txt`ï¼ˆä¹Ÿå¯ä»¥ç”¨å†…å­˜å­—ç¬¦ä¸²ï¼‰ï¼š

```
# Step 1: ä¿å­˜è¯­æ–™
with open("corpus.txt", "w", encoding="utf-8") as f:
    f.write("""zwei junge weiÃŸe mÃ¤nner sind im freien in der nÃ¤he vieler bÃ¼sche .
mehrere mÃ¤nner mit schutzhelmen bedienen ein antriebsradsystem .
ein kleines mÃ¤dchen klettert in ein spielhaus aus holz .
ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster .
zwei mÃ¤nner stehen am herd und bereiten essen zu .
ein mann in grÃ¼n hÃ¤lt eine gitarre , wÃ¤hrend der andere mann sein hemd ansieht .
ein mann lÃ¤chelt einen ausgestopften lÃ¶wen an .
ein schickes mÃ¤dchen spricht mit dem handy wÃ¤hrend sie langsam die straÃŸe entlangschwebt .
eine frau mit einer groÃŸen geldbÃ¶rse geht an einem tor vorbei .
jungen tanzen mitten in der nacht auf pfosten .""")
```

------

## ğŸ›  æ­¥éª¤äºŒï¼šè®­ç»ƒ BPE åˆ†è¯å™¨ï¼ˆè¯è¡¨å¤§å°è®¾ä¸º 100ï¼‰

```
from tokenizers import ByteLevelBPETokenizer

# Step 2: åˆå§‹åŒ–å¹¶è®­ç»ƒ BPE åˆ†è¯å™¨
tokenizer = ByteLevelBPETokenizer()
tokenizer.train(files=["corpus.txt"], vocab_size=100, min_frequency=1, special_tokens=["<pad>", "<unk>"])
```

> è¯´æ˜ï¼šæˆ‘ä»¬è®¾å®š `vocab_size=100`ï¼Œå°ä¸€äº›æ–¹ä¾¿ä½ è§‚å¯Ÿï¼ŒçœŸå®é¡¹ç›®ä¸­ä½ å¯ä»¥è®¾ç½®ä¸º 10,000 ç”šè‡³ 50,000ã€‚

------

## ğŸ‘ æ­¥éª¤ä¸‰ï¼šçœ‹çœ‹è¯è¡¨ä¸­å­¦åˆ°äº†å“ªäº› tokenï¼Ÿ

```
# Step 3: è¯è¡¨ä¸­å‰ 20 ä¸ª tokenï¼ˆå­è¯ï¼‰
vocab = tokenizer.get_vocab()
# æŒ‰ç…§ ID é¡ºåºæ’åº
sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])
for token, idx in sorted_vocab[:20]:
    print(f"{idx:3} â†’ {repr(token)}")
```

ä½ å°†çœ‹åˆ°åƒè¿™æ ·çš„è¾“å‡ºï¼ˆå†…å®¹ä¼šå› è®­ç»ƒç»“æœç•¥æœ‰ä¸åŒï¼‰ï¼š

```
  0 â†’ '<pad>'
  1 â†’ '<unk>'
  2 â†’ ' '
  3 â†’ 'ein'
  4 â†’ 'mann'
  5 â†’ 'in'
  6 â†’ 'der'
  7 â†’ 'zwei'
  8 â†’ 'mÃ¤'
  9 â†’ 'dchen'
 10 â†’ '.'
...
```

------

## âœ‚ï¸ æ­¥éª¤å››ï¼šå¯¹å¥å­è¿›è¡Œåˆ†è¯

æˆ‘ä»¬é€‰ä¸€è¡Œæµ‹è¯•ï¼š

```
# Step 4: å¯¹å¥å­è¿›è¡Œ BPE åˆ†è¯
output = tokenizer.encode("zwei mÃ¤nner stehen am herd und bereiten essen zu .")
print("Tokens:", output.tokens)
print("Token IDs:", output.ids)
```

è¾“å‡ºç±»ä¼¼äºï¼š

```
Tokens: ['zwei', ' mÃ¤nner', ' stehen', ' am', ' herd', ' und', ' bereiten', ' essen', ' zu', ' .']
Token IDs: [7, 15, 20, 18, 25, 12, 30, 29, 22, 10]
```

------

## âœ… æ€»ç»“ä½ åˆšåˆšåšäº†ä»€ä¹ˆï¼š

| æ“ä½œ               | è¯´æ˜                                                    |
| ------------------ | ------------------------------------------------------- |
| `vocab_size=100`   | é™åˆ¶æœ€ç»ˆè¯è¡¨æœ€å¤šä¿ç•™ 100 ä¸ª tokenï¼ˆåŒ…æ‹¬è¯ã€å­è¯å’Œå­—ç¬¦ï¼‰ |
| Tokenizer è®­ç»ƒè¿‡ç¨‹ | é€šè¿‡åˆå¹¶é¢‘ç¹å­—ç¬¦å¯¹ï¼Œå­¦ä¹ è¯æ ¹ã€è¯ç¼€ç­‰å­è¯                |
| è¾“å‡º token + ID    | æ˜¾ç¤ºæ¯ä¸ª tokenï¼ˆå­—ç¬¦ä¸²ï¼‰å’Œå…¶å¯¹åº”çš„å”¯ä¸€ ID               |

------

## ğŸ“¦ æƒ³è¯•æ›´å¤§çš„è¯è¡¨ï¼Ÿ

ä½ åªéœ€è¦ä¿®æ”¹è¿™ä¸€è¡Œï¼š

```
tokenizer.train(files=["corpus.txt"], vocab_size=10000, min_frequency=1)
```

ä½ å°†å¾—åˆ°æ›´ç»†è‡´ã€æ›´çµæ´»çš„åˆ†è¯èƒ½åŠ›ï¼Œå¹¶å‡å°‘ token æ•°é‡ã€‚

------

å¦‚ä½ éœ€è¦æˆ‘ç”¨ä½ çš„è¯­æ–™ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ `vocab.json` å’Œ `merges.txt` æ–‡ä»¶å¹¶å±•ç¤ºå…¶å†…å®¹ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ ç”Ÿæˆã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ

##### æ‚¨è¯´ï¼š

è¯è¡¨å˜å¤§äº†ï¼Œä¸ºä»€ä¹ˆ token æ•°é‡ä¼šå‡å°‘

###### ChatGPT è¯´ï¼š

è¿™ä¸ªé—®é¢˜éå¸¸å…³é”®ï¼Œè€Œä¸”æ­£è¯´æ˜ä½ å¯¹ Tokenizer å’Œæ¨¡å‹æ•ˆç‡çš„ç†è§£åœ¨åŠ æ·±ã€‚

------

## âœ… ä¸€å¥è¯å›ç­”ï¼š

> **è¯è¡¨å˜å¤§ï¼Œèƒ½åˆå¹¶å‡ºæ›´é•¿çš„ tokenï¼ˆæ›´å¤§çš„å­è¯ï¼‰ï¼Œæ‰€ä»¥å¤„ç†åŒä¸€å¥è¯æ—¶éœ€è¦çš„ token æ•°é‡å‡å°‘äº†ã€‚**

------

## ğŸ” åŸç†è§£æ

### BPE åˆ†è¯æ˜¯ã€Œç”±å°ç»„åˆæˆå¤§çš„ã€è¿‡ç¨‹ï¼š

- åˆå§‹é˜¶æ®µï¼šæ¯ä¸ªè¯æ‹†æˆå•ä¸ªå­—ç¬¦ï¼ˆç”šè‡³åŒ…æ‹¬ç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
- è®­ç»ƒé˜¶æ®µï¼š**ç»Ÿè®¡å‡ºç°é¢‘ç‡é«˜çš„ã€Œå­—ç¬¦å¯¹ã€è¿›è¡Œåˆå¹¶**
- è¯è¡¨è¶Šå¤§ï¼Œå°±å¯ä»¥**åˆå¹¶æ›´å¤šã€æ›´é•¿çš„å­è¯**ï¼Œç”šè‡³åˆå¹¶æˆå®Œæ•´å•è¯æˆ–è¯ç»„ï¼

------

## ğŸ§ª ä¸¾ä¸ªä¾‹å­ï¼ˆçœŸå®é€»è¾‘ï¼‰

ä»¥å¥å­ï¼š

```
zwei mÃ¤nner stehen am herd und bereiten essen zu .
```

### ğŸ”¹ å¦‚æœ `vocab_size = 100`ï¼ˆè¯è¡¨å¾ˆå°ï¼‰

å¯èƒ½è¢«åˆ†æˆï¼š

```
['z', 'w', 'ei', ' mÃ¤nner', ' stehen', ' a', 'm', ' h', 'erd', ' u', 'n', 'd', ' b', 'ere', 'iten', ' e', 'ssen', ' z', 'u', ' .']
ï¼ˆ20 ä¸ª tokenï¼‰
```

è¯è¡¨å°ï¼Œå¾ˆå¤šè¯åˆ†å¾—å¾ˆç¢ï¼Œæ‰€ä»¥ token å¤šã€‚

------

### ğŸ”¹ å¦‚æœ `vocab_size = 10000`ï¼ˆè¯è¡¨æ›´å¤§ï¼‰

å¯èƒ½å˜æˆï¼š

```
['zwei', ' mÃ¤nner', ' stehen', ' am', ' herd', ' und', ' bereiten', ' essen', ' zu', '.']
ï¼ˆ10 ä¸ª tokenï¼‰
```

è¯è¡¨å¤§ï¼Œå¸¸è§è¯éƒ½èƒ½ä½œä¸ºå•ç‹¬ token å­˜åœ¨ï¼Œ**åˆ†è¯ç»“æœæ›´é•¿ã€æ›´è¯­ä¹‰åŒ–ï¼Œtoken æ›´å°‘**ã€‚

------

## ğŸ“ˆ å›¾ç¤ºç†è§£ï¼ˆBPEåˆå¹¶è¿‡ç¨‹ï¼‰

åˆå§‹ token æ˜¯ï¼š

```
['a', 'n', 't', 'r', 'i', 'e', 'b', 's', 'r', 'a', 'd', 's', 'y', 's', 't', 'e', 'm']
```

BPE ä¼šä¸æ–­åˆå¹¶ï¼š

```
('s', 'y') â†’ 'sy'
('t', 'e') â†’ 'te'
('rad', 's') â†’ 'rads'
('system') â†’ 'system'
```

æœ€ç»ˆï¼š

```
['antriebsradsystem']
```

åŸæœ¬éœ€è¦ 17 ä¸ª tokenï¼Œç°åœ¨åªéœ€è¦ 1 ä¸ªï¼

------

## âœ… ä¸ºä»€ä¹ˆå‡å°‘ token æ•°é‡æ˜¯å¥½äº‹ï¼Ÿ

| ä¼˜åŠ¿                | è¯´æ˜                                     |
| ------------------- | ---------------------------------------- |
| ğŸš€ æ›´çŸ­çš„ token åºåˆ— | å¤„ç†æ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘ï¼Œèƒ½æ”¾è¿›æ›´é•¿æ–‡æœ¬   |
| ğŸ§  æ›´å¥½çš„è¯­ä¹‰è¡¨è¾¾    | æ¯ä¸ª token æ›´å…·â€œè¯â€æ„ä¹‰ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£    |
| ğŸ“Š æ›´ç¨³å®šçš„è®­ç»ƒ      | é¿å…ç¨€ç–å­—ç¬¦æˆ–è¯è¢«ç‹¬ç«‹å»ºæ¨¡ï¼Œæå‡æ³›åŒ–èƒ½åŠ› |

------

## âš ï¸ æ³¨æ„ï¼šè¯è¡¨å¤ªå¤§ä¹Ÿæœ‰é£é™©ï¼

- æ¨¡å‹ embedding å±‚å‚æ•°é‡å¤§ï¼ˆæ¯ä¸ª token éƒ½æœ‰å‘é‡ï¼‰
- ç¨€æœ‰è¯ token å¤ªå¤šï¼Œæ³›åŒ–èƒ½åŠ›å¯èƒ½ä¸‹é™
- å¯¹ä½èµ„æºè¯­æ–™è¿‡æ‹Ÿåˆé£é™©å¢åŠ 

------

## âœ… æ€»ç»“

| è¯è¡¨å¤§å°ï¼ˆvocab_sizeï¼‰ | åˆ†è¯ç²’åº¦         | Token æ•°é‡ï¼ˆæ¯å¥ï¼‰ | æ¨¡å‹æ•ˆç‡         |
| ---------------------- | ---------------- | ------------------ | ---------------- |
| å°ï¼ˆå¦‚ 500ï¼‰           | æ‹†å¾—ç»†ï¼ˆå­—ç¬¦çº§ï¼‰ | å¤š                 | ä½æ•ˆï¼Œç†è§£å·®     |
| ä¸­ç­‰ï¼ˆå¦‚ 10k~50kï¼‰     | å­è¯çº§           | å¹³è¡¡               | âœ… æ¨èä½¿ç”¨       |
| å¤ªå¤§ï¼ˆå¦‚ >100kï¼‰       | è¶‹è¿‘æ•´è¯         | å°‘ï¼Œä½†ç¨€ç–         | å‚æ•°å¤§ï¼Œæ˜“è¿‡æ‹Ÿåˆ |

------

æ˜¯å¦æƒ³è®©æˆ‘å¸®ä½ ç”»ä¸€ä¸ªã€Œè¯è¡¨å¤§å° vs tokenæ•°é‡ vs æ¨¡å‹æ•ˆç‡ã€çš„ç¤ºæ„å›¾ï¼Ÿè§†è§‰åŒ–å¯èƒ½ä¼šæ›´æ¸…æ¥šã€‚

##### æ‚¨è¯´ï¼š

tokenizerså¦‚ä½•å®‰è£…

###### ChatGPT è¯´ï¼š

ä½ å¯ä»¥ä½¿ç”¨ `pip` ç›´æ¥å®‰è£… `tokenizers`ï¼Œæ–¹æ³•éå¸¸ç®€å•ï¼Œä¸‹é¢æ˜¯å®Œæ•´è¯´æ˜ï¼š

------

## âœ… ä¸€å¥è¯å®‰è£…å‘½ä»¤ï¼š

```
pip install tokenizers
```

------

## ğŸ“Œ è¯´æ˜ï¼š

| é¡¹ç›®        | å†…å®¹                                                |
| ----------- | --------------------------------------------------- |
| åº“åç§°      | `tokenizers`                                        |
| å¼€å‘è€…      | Hugging Faceï¼ˆä¹Ÿæ˜¯ transformers çš„å¼€å‘è€…ï¼‰          |
| æ”¯æŒå¹³å°    | Windows / macOS / Linux                             |
| Python ç‰ˆæœ¬ | Python â‰¥ 3.6                                        |
| ç‰¹ç‚¹        | é«˜æ€§èƒ½ï¼ˆåº•å±‚ç”¨ Rust ç¼–å†™ï¼‰ï¼Œå…¼å®¹ BPE / WordPiece ç­‰ |

------

## ğŸ§ª éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸï¼š

å®‰è£…å®Œæˆåï¼Œåœ¨ Python ä¸­æµ‹è¯•ï¼š

```
from tokenizers import ByteLevelBPETokenizer

tokenizer = ByteLevelBPETokenizer()
print("å®‰è£…æˆåŠŸï¼Œtokenizer å¯ç”¨ï¼")
```

å¦‚æœæ²¡æœ‰æŠ¥é”™ï¼Œå°±æ˜¯å®‰è£…æˆåŠŸã€‚

------

## âš ï¸ å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ³•ï¼š

### âŒ æŠ¥é”™ï¼šæ‰¾ä¸åˆ° Rust ç¼–è¯‘å™¨

`tokenizers` æ˜¯ Rust å†™çš„ï¼Œæœ‰æ—¶ Windows ç¯å¢ƒä¸‹ `pip` ç¼–è¯‘å¤±è´¥ã€‚è§£å†³æ–¹å¼å¦‚ä¸‹ï¼š

### âœ… æ–¹æ³•ä¸€ï¼šç›´æ¥ç”¨ `pip` å®‰è£…é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ˆæ¨èï¼‰

ç¡®ä¿ä½¿ç”¨çš„æ˜¯ **æœ€æ–°ç‰ˆæœ¬çš„ pip**ï¼š

```
pip install --upgrade pip
pip install tokenizers
```

### âœ… æ–¹æ³•äºŒï¼šå¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå¯ä»¥å°è¯•å®‰è£… wheel åŒ…ï¼š

```
pip install tokenizersâ€‘0.14.1â€‘cp39â€‘cp39â€‘win_amd64.whl
```

> ä½ å¯ä»¥ä»è¿™é‡Œä¸‹è½½å¯¹åº” Python ç‰ˆæœ¬çš„ `.whl` æ–‡ä»¶ï¼ˆå¦‚ Python 3.9ï¼‰ï¼š
> https://www.lfd.uci.edu/~gohlke/pythonlibs/#tokenizers

------

## ğŸ§± é™„åŠ æ¨èï¼š

å¦‚æœä½ è¦åšå®Œæ•´çš„ NLP æ¨¡å‹åˆ†è¯ï¼Œå¯ä»¥ä¸€èµ·è£…ï¼š

```
pip install transformers
```

è¿™æ ·ä½ å¯ä»¥åŒæ—¶ä½¿ç”¨ï¼š

- `tokenizers`ï¼ˆåº•å±‚å¿«ï¼‰
- `transformers`ï¼ˆä¸Šå±‚æ¨¡å‹+é¢„è®­ç»ƒTokenizerï¼‰

------

## âœ… æ€»ç»“ï¼š

| ä»»åŠ¡                        | å‘½ä»¤                        |
| --------------------------- | --------------------------- |
| å®‰è£… `tokenizers`           | `pip install tokenizers`    |
| å®‰è£…æœ€æ–°ç‰ˆ pip              | `pip install --upgrade pip` |
| å®‰è£… `transformers`ï¼ˆå¦‚éœ€ï¼‰ | `pip install transformers`  |

------

å¦‚æœä½ å‘Šè¯‰æˆ‘ä½ çš„ Python ç‰ˆæœ¬å’Œæ“ä½œç³»ç»Ÿï¼Œæˆ‘å¯ä»¥ç»™ä½ ä¸“é—¨é€‚é…çš„å®‰è£…å‘½ä»¤ã€‚éœ€è¦å—ï¼Ÿ

##### æ‚¨è¯´ï¼š

init__.pyi' ä¸­æ‰¾ä¸åˆ°å¼•ç”¨ 'ByteLevelBPETokenizer' 

###### ChatGPT è¯´ï¼š

ä½ é‡åˆ°çš„é”™è¯¯ï¼š

> ```
> __init__.pyi ä¸­æ‰¾ä¸åˆ°å¼•ç”¨ 'ByteLevelBPETokenizer'
> ```

è¯´æ˜ä½ å¯èƒ½åœ¨ç”¨ **è‡ªåŠ¨è¡¥å…¨å·¥å…·ï¼ˆå¦‚ PyCharmã€VSCodeï¼‰** æˆ– **ç±»å‹æç¤ºæ£€æŸ¥å™¨ï¼ˆå¦‚ mypyï¼‰** æ—¶ï¼Œæ‰¾ä¸åˆ° `ByteLevelBPETokenizer` è¿™ä¸ªç±»çš„ç±»å‹å®šä¹‰ã€‚

------

## âœ… è§£å†³æ–¹æ¡ˆä¸€ï¼šç¡®è®¤å¯¼å…¥è·¯å¾„æ˜¯å¦æ­£ç¡®

åœ¨ `tokenizers` ä¸­ï¼Œæœ‰ä¸¤ä¸ªå¸¸è§çš„ä½¿ç”¨æ–¹å¼ï¼Œè¯·ç¡®ä¿ä½ ç”¨çš„æ˜¯**æ­£ç¡®çš„å¯¼å…¥æ–¹å¼**ï¼š

### âœ… æ­£ç¡®å¯¼å…¥æ–¹å¼æ˜¯ï¼š

```
from tokenizers import ByteLevelBPETokenizer
```

æˆ–ï¼ˆæ›´æ¨èè¿™ç§æ›´å®‰å…¨çš„å†™æ³•ï¼‰ï¼š

```
from tokenizers.implementations import ByteLevelBPETokenizer
```

> `tokenizers.implementations` æ˜¯å®é™…å®šä¹‰ç±»çš„ä½ç½®ï¼ŒIDE ç±»å‹æç¤ºæ›´å‹å¥½ã€‚

------

## âœ… è§£å†³æ–¹æ¡ˆäºŒï¼šå¼ºåˆ¶å‡çº§ tokenizers åŒ…

æœ‰æ—¶æ—§ç‰ˆæœ¬çš„ `tokenizers` ç¼ºå°‘ `.pyi` ç±»å‹æç¤ºæ–‡ä»¶ã€‚è¯·å…ˆå‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬ï¼š

```
pip install --upgrade tokenizers
```

ç¡®è®¤ç‰ˆæœ¬ï¼š

```
pip show tokenizers
```

è¾“å‡ºä¸­åº”æ˜¾ç¤ºç‰ˆæœ¬å¦‚ `0.14.x`ï¼ˆæˆ–æ›´æ–°ï¼‰ï¼Œå¦‚æœä½ æ˜¯ 0.10.x æˆ–æ›´ä½ï¼Œå»ºè®®å‡çº§ã€‚

------

## âœ… è§£å†³æ–¹æ¡ˆä¸‰ï¼ˆIDE ä¸“å±ï¼‰ï¼š

### å¦‚æœä½ åœ¨ä½¿ç”¨ **PyCharm / VSCode**ï¼š

- å¯èƒ½ IDE æ²¡æœ‰æ­£ç¡®ç´¢å¼•ä¾èµ–ï¼Œå°è¯•ï¼š
  - æ¸…é™¤ç¼“å­˜ï¼ˆPyCharm: `File > Invalidate Caches and Restart`ï¼‰
  - é‡å¯ IDE
  - é‡æ–°å®‰è£…è§£é‡Šå™¨æˆ–åˆ·æ–°è™šæ‹Ÿç¯å¢ƒä¾èµ–

------

## âœ… ä»£ç ç¤ºä¾‹ï¼ˆæ¨èå†™æ³•ï¼‰ï¼š

```
# âœ… æ¨èå†™æ³•ï¼Œé¿å… IDE è­¦å‘Š
from tokenizers.implementations import ByteLevelBPETokenizer

tokenizer = ByteLevelBPETokenizer()
tokenizer.train(files=["corpus.txt"], vocab_size=100, min_frequency=1)

print(tokenizer.encode("zwei mÃ¤nner stehen am herd").tokens)
```

------

## âœ… æ€»ç»“ï¼š

| é—®é¢˜åŸå›                                | è§£å†³æ–¹æ³•                                 |
| -------------------------------------- | ---------------------------------------- |
| ç±»å‹æç¤ºæ‰¾ä¸åˆ° `ByteLevelBPETokenizer` | ç”¨ `tokenizers.implementations` æ¥å¯¼å…¥   |
| IDE è‡ªåŠ¨è¡¥å…¨å¤±æ•ˆ                       | é‡å¯ IDE / æ¸…é™¤ç¼“å­˜                      |
| åŒ…å¤ªè€ç¼ºå°‘ `.pyi` æ–‡ä»¶                 | å‡çº§ï¼š`pip install --upgrade tokenizers` |

------

å¦‚æœä½ å‘Šè¯‰æˆ‘ä½ ä½¿ç”¨çš„æ˜¯å“ªç§ç¼–è¾‘å™¨ï¼ˆVSCodeï¼ŸPyCharmï¼ŸJupyterï¼Ÿï¼‰å’Œ Python ç‰ˆæœ¬ï¼Œæˆ‘å¯ä»¥ä¸ºä½ å®šåˆ¶æ›´å…·ä½“çš„è§£å†³æ–¹å¼ã€‚éœ€è¦æˆ‘å¸®ä½ ï¼Ÿ







ChatGPT ä¹Ÿå¯èƒ½ä¼šçŠ¯é”™ã€‚è¯·æ ¸æŸ¥é‡è¦ä¿¡æ¯ã€‚