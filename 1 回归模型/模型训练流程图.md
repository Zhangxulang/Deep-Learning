### ✅ Mermaid 深度学习训练流程图（结合 PyTorch）

1. **数据准备阶段**：
   - 数据预处理：归一化、数据增强等
   - 划分数据集：典型比例为 60%/20%/20% 或 70%/15%/15%
2. **模型构建阶段**：
   - 网络结构：CNN/RNN/Transformer等
   - 优化器选择：Adam/SGD等
   - 损失函数：交叉熵/MSE等
3. **训练循环**：
   - 前向传播：计算预测值
   - 损失计算：比较预测和真实值
   - 反向传播：计算梯度
   - 参数更新：通过优化器调整权重
4. **验证与测试**：
   - 验证集用于早停和超参数调整
   - 测试集仅用于最终评估
5. **模型保存**：
   - 保存训练好的权重
   - 保存整个模型架构

### 每个阶段 PyTorch 示例代码

#### 1. 数据准备

```python
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

------

#### 2. 构建模型

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 32, 3, 1)
        self.fc = nn.Linear(5408, 10)
        
    def forward(self, x):
        x = F.relu(self.conv(x))
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x
```

------

#### 3. 训练过程

```python
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

EPOCHS = 10
for epoch in range(EPOCHS):
    model.train()
    for batch_x, batch_y in train_loader:
        output = model(batch_x)
        loss = criterion(output, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}: Loss = {loss.item()}")
```

------

#### 4. 模型保存与加载

```python
# 保存模型
torch.save(model.state_dict(), "cnn_model.pth")

# 加载模型
model.load_state_dict(torch.load("cnn_model.pth"))
model.eval()
```

------

###  可选增强

- `torch.no_grad()`：验证/推理中关闭梯度计算
- `lr_scheduler.StepLR`：动态调整学习率。
- `early stopping`：在验证集性能长期不提升时提前终止训练。
- `TensorBoard`：记录 loss/accuracy 曲线，便于可视化分析。